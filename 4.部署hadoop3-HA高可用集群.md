## 1.HA整体架构

> ![image](https://github.com/wrt199399/My-BigData-Learning-Trip/blob/master/images/4/HA-1.png)

架构梳理：

​	两台NameNode互为主备，通过zk集群来进行注册协调，并通过zkfc来防止发生脑裂

JN集群来记录namenode元数据日志信息，并在两台NN之间进行数据透传；

DN集群存储block数据。



各进程服务器分布图：

> ![image](https://github.com/wrt199399/My-BigData-Learning-Trip/blob/master/images/4/HA-2.png)



## 2.配置hadoop3-HA

从hadoop官网可以寻找配置文档

> ![image](https://github.com/wrt199399/My-BigData-Learning-Trip/blob/master/images/4/HA-3.png)

1. 将之前的HA配置文件夹复制一份，并改名为hadoop-full

2. 修改hadoop下的配置文件

   1. 修改hadoop-env.sh

      ```shell
      export JAVA_HOME=/root/app/jdk1.8.0_141
      export HDFS_NAMENODE_USER=root
      export HDFS_DATANODE_USER=root
      #export HDFS_SECONDARYNAMENODE_USER=root
      export HDFS_ZKFC_USER=root
      export HDFS_JOURNALNODE_USER=root
      ```

   2. 修改core-site.xml

      ```xml
      <property>
              <name>fs.defaultFS</name>
              <value>hdfs://mycluster</value>
      </property>
      <property>
              <name>hadoop.tmp.dir</name>
              <value>/root/var/hadoop/ha</value>
      </property>
      <property>
              <name>hadoop.http.staticuser.user</name>
              <value>root</value>
      </property>
      <property>
             <name>ha.zookeeper.quorum</name>
             <value>node02:2181,node03:2181,node04:2181</value>
      </property>
      ```

   3. 修改hdfs-site.xml

      ```xml
      <property>
               <name>dfs.replication</name>
               <value>2</value>
      </property>
      <property>
               <name>dfs.nameservices</name>
               <value>mycluster</value>
      </property>
      <property>
               <name>dfs.ha.namenodes.mycluster</name>
               <value>nn1,nn2</value>
      </property>
      <property>
               <name>dfs.namenode.rpc-address.mycluster.nn1</name>
               <value>node01:8020</value>
      </property>
      <property>
               <name>dfs.namenode.rpc-address.mycluster.nn2</name>
               <value>node02:8020</value>
      </property>
      
      <property>
               <name>dfs.namenode.http-address.mycluster.nn1</name>
               <value>node01:9870</value>
      </property>
      <property>
               <name>dfs.namenode.http-address.mycluster.nn2</name>
               <value>node02:9870</value>
      </property>
      <!--journalNode集群的配置-->
      <property>
       <name>dfs.namenode.shared.edits.dir</name>    <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
      </property>
      <!--zkfc故障转移的代理类配置-->
      <property>              <name>dfs.client.failover.proxy.provider.mycluster</name>                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
      <!--隔离方法-->
      <property>
           <name>dfs.ha.fencing.methods</name>
           <value>sshfence</value>
      </property>
      <!--私钥路径-->
      <property>
             <name>dfs.ha.fencing.ssh.private-key-files</name>
             <value>/root/.ssh/id_rsa</value>
      </property>
      <property>
             <name>dfs.journalnode.edits.dir</name>
             <value>/root/var/hadoop/ha/journalnode</value>
      </property>
      <!--是否启用自动故障转移-->
      <property>
             <name>dfs.ha.automatic-failover.enabled</name>
             <value>true</value>
      </property>
      ```

      

3. 将配置文件分发到其他节点



## 3. 安装zookeeper

1. 复制安装包zookeeper3.4.6

2. 解压

3. 配置环境变量

4. 修改配置 conf/zoo.cfg

   ```shell
   # the directory where the snapshot is stored.
   # do not use /tmp for storage, /tmp here is just 
   # example sakes.
   dataDir=/var/zk
   # the port at which the clients will connect
   clientPort=2181
   # the maximum number of client connections.
   # increase this if you need to handle more clients
   #maxClientCnxns=60
   #
   # Be sure to read the maintenance section of the 
   # administrator guide before turning on autopurge.
   #
   # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
   #
   # The number of snapshots to retain in dataDir
   #autopurge.snapRetainCount=3
   # Purge task interval in hours
   # Set to "0" to disable auto purge feature
   #autopurge.purgeInterval=1
   server.1=node02:2888:3888
   server.2=node03:2888:3888
   server.3=node04:2888:3888
   ```

5. 将配置分发到node02,node03,node04

6. 将服务器编号写入文件当中

   ```shell
   node02,03,04 都操作： mkdir -p /var/zk
   node01操作： echo 1> /var/zk/myid
   node02操作： echo 2> /var/zk/myid
   node03操作： echo 3> /var/zk/myid
   ```

7. zk运行

   ```shell
   3台服务器操作 ： zkServer.sh start
   查看状态：      zkServer.sh status
   ```



## 4.NameNode同步透穿

1. 启动JN,分别在02,02,03

   ```
   hdfs --daemon start journalnode
   ```

2. 进行namenode的格式化

   ```
   node01执行： hdfs namenode -format
   ```

3. 启动当前namenode01节点

   ```
   node01执行： hdfs --daemon start
   ```

4. 在node02上进行同步

   ```
   node02执行：hdfs namenode -bootstrapStandby
   ```



## 5.zk格式化，启动ha及测试

1. 在zk上注册节点目录

   ```
   hdfs zkfc -formatZK
   ```

2. 启动集群

   ```
   start-dfs.sh
   ```

3. 测试某主节点宕机

   ```
   关掉namenode01： hdfs --daemon stop namenode
   ```

4. 测试zk网络中断看强制将节点降级是否管用

   ```
   关掉namenode02的zkfc:  hdfs --daemon stop zkfc
   ```



### hadoop3 HA部署完成！

